{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset and split the data set into training/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training/test data\n",
    "%store -r X_train\n",
    "%store -r X_test\n",
    "%store -r y_train\n",
    "%store -r y_test\n",
    "%store -r X_train_scaled\n",
    "%store -r X_test_scaled \n",
    "%store -r X_train_stand\n",
    "%store -r X_test_stand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the optimized model\n",
    "The final optimized random forest model has the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8958810068649885\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "filename = 'random_forest.sav'\n",
    "model = joblib.load(filename)\n",
    "result = model.score(X_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "     CANDIDATE       0.82      0.77      0.80       422\n",
      "     CONFIRMED       0.80      0.82      0.81       450\n",
      "FALSE POSITIVE       0.98      1.00      0.99       876\n",
      "\n",
      "      accuracy                           0.90      1748\n",
      "     macro avg       0.87      0.86      0.86      1748\n",
      "  weighted avg       0.89      0.90      0.90      1748\n",
      "\n",
      "89.59%\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n",
    "print(f\"{round(model.score(X_test, y_test)*100,2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>CANDIDATE</td>\n",
       "      <td>CANDIDATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5609</th>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6558</th>\n",
       "      <td>CANDIDATE</td>\n",
       "      <td>CANDIDATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>CONFIRMED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247</th>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6859</th>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1687</th>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>CONFIRMED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6956</th>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3078</th>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>CANDIDATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4304</th>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2493</th>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>CONFIRMED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3618</th>\n",
       "      <td>CANDIDATE</td>\n",
       "      <td>CANDIDATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5131</th>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4016</th>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Actual       Predicted\n",
       "1981       CANDIDATE       CANDIDATE\n",
       "5609  FALSE POSITIVE  FALSE POSITIVE\n",
       "532   FALSE POSITIVE  FALSE POSITIVE\n",
       "6558       CANDIDATE       CANDIDATE\n",
       "1249  FALSE POSITIVE  FALSE POSITIVE\n",
       "237        CONFIRMED       CONFIRMED\n",
       "3247  FALSE POSITIVE  FALSE POSITIVE\n",
       "6859  FALSE POSITIVE  FALSE POSITIVE\n",
       "1687       CONFIRMED       CONFIRMED\n",
       "1143       CONFIRMED  FALSE POSITIVE\n",
       "6956  FALSE POSITIVE  FALSE POSITIVE\n",
       "3078  FALSE POSITIVE  FALSE POSITIVE\n",
       "3945       CONFIRMED       CANDIDATE\n",
       "4304  FALSE POSITIVE  FALSE POSITIVE\n",
       "2493       CONFIRMED       CONFIRMED\n",
       "3618       CANDIDATE       CANDIDATE\n",
       "281   FALSE POSITIVE  FALSE POSITIVE\n",
       "5131  FALSE POSITIVE  FALSE POSITIVE\n",
       "4016  FALSE POSITIVE  FALSE POSITIVE\n",
       "647   FALSE POSITIVE  FALSE POSITIVE"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df = pd.DataFrame({\"Actual\": y_test, \"Predicted\":predictions})\n",
    "prediction_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of total predictions: 1748\n",
      "The number of correct predictions: 1566\n",
      "The test test accuracy: 89.59%\n"
     ]
    }
   ],
   "source": [
    "prediction_df['match'] = np.where(prediction_df['Predicted'] == prediction_df['Actual'], 1, 0)\n",
    "print(f\"The number of total predictions: {len(prediction_df)}\")\n",
    "print(f\"The number of correct predictions: {sum(prediction_df['match'])}\")\n",
    "print(f\"The test test accuracy: {round(sum(prediction_df['match'])/len(prediction_df)*100,2)}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
